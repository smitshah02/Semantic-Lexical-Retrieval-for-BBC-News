# Data Documentation

This document describes the raw data, processed artifacts, and evaluation files used in the Semantic–Lexical Retrieval System.

---

# 1. Raw BBC Dataset (`raw_data/bbc/`)

Contains 5 topic folders:

```
business/
entertainment/
politics/
sport/
tech/
```

Each folder contains plain-text articles.
Each file includes:
- a title (first non-empty line)
- article body text
- category inferred from the folder name

---

# 2. Corpus (`artifacts/bbc_corpus.csv`)

Generated by `build_corpus.py`.

Columns:
- `id` : unique document identifier  
- `title`  
- `text`  
- `category`  

Represents cleaned article-level data.

---

# 3. Chunked Corpus (`artifacts/bbc_chunks.jsonl`)

Generated by `build_chunks.py`.

Chunks are ~450–600 words with ~100-word overlap.

Each entry contains:
- `id` : chunk ID  
- `doc_id` : original article ID  
- `chunk_index`  
- `title`  
- `category`  
- `text` : chunk text  

Used by both lexical and semantic models.

---

# 4. Lexical Artifacts (`artifacts/`)

Generated by `build_lexical.py`:

### `tfidf.pkl`
- fitted TfidfVectorizer  
- document-term matrix  
- chunk IDs  

### `bm25.pkl`
- tokenized chunk corpus  
- BM25 scorer  
- chunk IDs  

Used by:
- TFIDFRetriever  
- BM25Retriever  

---

# 5. Semantic Artifacts (`artifacts/`)

Generated by `build_embeddings.py`:

### `embeddings.npy`
Dense semantic embeddings for each chunk.

### `doc_ids.json`
Chunk ID list aligned with embedding rows.

### `faiss.index` (optional)
FAISS-based nearest-neighbor index.

Used by:
- SemanticRetriever  

---

# 6. Evaluation Data (`data/`)

### `queries.tsv`
Columns:
- `qid`
- `query`

### `qrels.tsv`
Relevance judgments:
- `qid`
- `docid`
- `relevance` (≥1 is considered relevant)

Used in:
```
src/evaluate.py
```

---

# 7. Reproducibility Notes

- All processed data can be regenerated from raw BBC files.  
- Ensure raw data is placed under `raw_data/bbc/` before running build scripts.  
- The notebook demo uses only the artifacts, not raw text files.